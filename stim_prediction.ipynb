{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downstream: Predicting Stimuli Type from Scanpath\n",
    "\n",
    "- Author: Beibin Li\n",
    "- Date: Sept/20/2021\n",
    "- Refer to Section 4.1 of [arxiv/2108.05025](https://arxiv.org/abs/2108.05025)\n",
    "\n",
    "\n",
    "Here, we perform naive k-shot n-way supervised learning method.\n",
    "\n",
    "Note that the FixaTon dataset belongs to MIT, and you need to download this MIT1003 dataset from [here](http://people.csail.mit.edu/tjudd/WherePeopleLook/index.html).\n",
    "\n",
    "Then, you can use the sample_data/preprocess_FixaTons.py code to pre-process the scanpath data to a compatible format.\n",
    "You can check \"sample_data/FixaTons/MIT1003/clean_data\" to see some example cleaned scanpaths.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import OBF libraries\n",
    "from obf.model import ae\n",
    "from obf.model import creator\n",
    "from obf.dataloader.augmenter import train_transform, valid_transform\n",
    "from obf.dataloader.mit_loader import MITDataset\n",
    "\n",
    "from obf.utils.metrics import top_k_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import torch, numpy, scipy, and other libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tqdm\n",
    "import termcolor\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will setup the path and training parameters.\n",
    "\n",
    "The MIT1003 dataset should be downloaded and cleaned in the `INPUT_DATA_DIR` folder.\n",
    "The pre-trained model should be saved in the `PRE_TRAIN_DIR` folder.\n",
    "\n",
    "We perform `WAYS`-classificaiton with `WAYS` classes, and we have `SHOTS` examples (scanpaths) for each of the class.\n",
    "\n",
    "The `BATCH_SIZE`, `LEARNING_RATE`, and `EPOCHS` are arbitrary settings for the deep learning experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@Parameters Settings\n",
    "INPUT_DATA_DIR = \"../gaze_data/FixaTons/MIT1003/clean_data/\"  #@param\n",
    "PRE_TRAIN_DIR = \"pre_weights/sample_weights/\"  #@param\n",
    "OUTPUT_DIR = \"downstream_cache/\"  #@param\n",
    "\n",
    "SHOTS = 5  #@param\n",
    "WAYS = 100  #@param\n",
    "\n",
    "BATCH_SIZE = 4  #@param\n",
    "LEARNING_RATE = 0.001  #@param\n",
    "EPOCHS = 100  #@param\n",
    "REPORT_INTERVAL = 10  #@param\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_TYPE should either be \"freeze\" or \"tune\",\n",
    "# if \"freeze\", we will freeze the Conv/RNN pre-trained encoder, and only fine-tune the new FC layers in the classifier\n",
    "# if \"tune\", we will fine-tune the whole model.\n",
    "\n",
    "TRAIN_TYPE = \"tune\"  #@param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Create and setup output path\n",
    "time_str = datetime.datetime.now().strftime(\"%Y_%m_%d_%H.%M.%S\")\n",
    "\n",
    "checkpt_name = os.path.join(OUTPUT_DIR, time_str + \".txt\")\n",
    "log_dirname = os.path.join(OUTPUT_DIR, time_str + \"_log\")\n",
    "model_save_path = os.path.join(log_dirname, \"model.pt\")\n",
    "\n",
    "\n",
    "os.makedirs(log_dirname, exist_ok=True)\n",
    "\n",
    "\n",
    "# Tensorboard writer\n",
    "summary_writer = SummaryWriter(log_dir = log_dirname, flush_secs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model from Pre-Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title We can load the pre-trained encoder first\n",
    "encoder = creator.load_encoder(PRE_TRAIN_DIR, use_cuda=USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Create a classifier model, which concatenated FC layers after the encoder.\n",
    "model = creator.create_classifier_from_encoder(encoder, hidden_layers=[256, 512], n_output=WAYS, dropout=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator.print_models_info([\"original encoder\", \"current model\"], [encoder, model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CUDA:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "train_ds = MITDataset(mode=\"train\", folder_name=INPUT_DATA_DIR, aug_transform=train_transform, shot=SHOTS, way=WAYS)\n",
    "valid_ds = MITDataset(mode=\"valid\", folder_name=INPUT_DATA_DIR, aug_transform=valid_transform, shot=SHOTS, way=WAYS)\n",
    "test_ds = MITDataset(mode=\"test\", folder_name=INPUT_DATA_DIR, aug_transform=valid_transform, shot=SHOTS, way=WAYS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader  = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, \n",
    "                       shuffle=True, pin_memory=True, num_workers=0)  \n",
    " \n",
    "valid_loader  = torch.utils.data.DataLoader(valid_ds, batch_size=BATCH_SIZE, \n",
    "                       shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "test_loader  = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE, \n",
    "                       shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "print(\"Data Loader Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(mode, model, dataloader, writer, epoch_id, optimizer=None):\n",
    "  # mode is either \"train\" or \"test\" or \"valid\"\n",
    "  assert( epoch_id is not None )\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  if mode == \"train\": \n",
    "    model = model.train( True )\n",
    "  else:\n",
    "    model.eval()\n",
    "    \n",
    "  epoch_losses = []\n",
    "  \n",
    "  reals = []\n",
    "  preds = []\n",
    "  probs = []\n",
    "  for signal, label in dataloader:\n",
    "    signal = signal.float()\n",
    "    label = label.long().reshape(-1)\n",
    "    # pdb.set_trace()\n",
    "    if USE_CUDA:  \n",
    "      signal = signal.cuda()\n",
    "      label = label.cuda()\n",
    "\n",
    "    # pdb.set_trace()\n",
    "\n",
    "    if mode == \"train\": \n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      if signal.shape[0] < 2:\n",
    "        continue # batch norm needs more than 1 sample\n",
    " \n",
    "    # forward + backward + optimize\n",
    "    outputs = model(signal)\n",
    "    loss = criterion(outputs, label)\n",
    "    epoch_losses.append(loss.item())\n",
    "\n",
    "    reals += label.cpu().numpy().tolist()\n",
    "    preds += torch.argmax(outputs, dim=1).detach().cpu().numpy().tolist()\n",
    "    probs.append(outputs.detach())\n",
    "\n",
    "    if mode == \"train\":\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "  total_loss = np.nanmean(epoch_losses)\n",
    "  acc = np.sum(np.array(reals) == np.array(preds)) / len(reals)\n",
    "  \n",
    "  probs = torch.cat(probs, dim=0)\n",
    "  top_5_acc = top_k_accuracy(probs.detach().cpu().numpy(), reals, k=5)\n",
    "  \n",
    "  f1 = f1_score(reals, preds, average=\"weighted\")\n",
    "  # auc = roc_auc_score(reals, probs, average=\"weighted\")\n",
    "\n",
    "  \n",
    "  writer.add_scalar(mode + \"/loss\", total_loss, epoch_id)\n",
    "  writer.add_scalar(mode + \"/acc\", acc, epoch_id)\n",
    "  writer.add_scalar(mode + \"/top_5_acc\", top_5_acc, epoch_id)\n",
    "  writer.add_scalar(mode + \"/f1\", f1, epoch_id)\n",
    "  # writer.add_scalar(mode + \"/auc\", auc, epoch_id)\n",
    "  writer.file_writer.flush()\n",
    "  \n",
    "  # print(\"#\" * 50)\n",
    "  if epoch_id % REPORT_INTERVAL == 0 or epoch_id == EPOCHS - 1:\n",
    "    msg = \"#\" * 5 + \"%s, Epoch: %d, Accuracy: %.2f, F-1: %.2f, Top-5: %.2f; Loss: %.2f\" % (mode, \n",
    "             epoch_id, acc, f1, top_5_acc, total_loss)\n",
    "\n",
    "    if mode == \"train\":\n",
    "      color = \"red\"\n",
    "    elif mode == \"valid\":\n",
    "      color = \"yellow\"\n",
    "    else:\n",
    "      color = \"green\"\n",
    "\n",
    "    print(termcolor.colored(msg, color=color))\n",
    " \n",
    "    # print(\"reals\", reals, \"preds\", preds)\n",
    "    # print(confusion_matrix(reals, preds))\n",
    "  \n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup the optimizer based on \"freeze\" or \"tune\"\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "if TRAIN_TYPE == \"freeze\":\n",
    "  optimizer = optim.Adam(model[1:].parameters(), lr=LEARNING_RATE) # Freeze the Encoder\n",
    "  print(termcolor.colored(\"We will FREEZE the encoder.\", \"blue\"))\n",
    "elif TRAIN_TYPE == \"tune\":\n",
    "  print(termcolor.colored(\"We will TUNE the WHOLE model.\", \"blue\"))\n",
    "else:\n",
    "  raise \"Unknown mode. It should be one of (tune, freeze, new)\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Begin Training \n",
    "train_accs = []\n",
    "valid_accs = []\n",
    "test_accs = []\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=max(1, int(EPOCHS / 5)), gamma=0.5)\n",
    "\n",
    "for epoch in tqdm.tqdm(range(EPOCHS)):\n",
    "  auc = run_model(\"train\", model, train_loader, summary_writer, epoch_id = epoch, optimizer=optimizer)\n",
    "  summary_writer.add_scalar( \"train/learning_rate\", scheduler.get_last_lr()[-1], epoch )\n",
    "\n",
    "  scheduler.step()\n",
    "\n",
    "  torch.save(model, model_save_path)\n",
    "\n",
    "  with torch.no_grad():\n",
    "     valid_acc = run_model(\"valid\", model, valid_loader, summary_writer, epoch_id=epoch)\n",
    "     test_acc = run_model(\"test\", model, test_loader, summary_writer, epoch_id=epoch)\n",
    "     train_accs.append(auc)\n",
    "     valid_accs.append(valid_acc)\n",
    "     test_accs.append(test_acc)\n",
    "\n",
    "\n",
    "times = range(len(train_accs))\n",
    "plt.plot(times, train_accs, color=\"blue\", label=\"train\", alpha=0.5)\n",
    "plt.plot(times, valid_accs, color=\"yellow\", label=\"valid\", alpha=0.5)\n",
    "plt.plot(times, test_accs, color=\"red\", label=\"test\", alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
